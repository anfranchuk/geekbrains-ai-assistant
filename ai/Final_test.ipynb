{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-25T07:24:04.359551Z",
     "start_time": "2023-11-25T07:23:50.834566Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nikolaibabuhin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from ai.lecture_notes.llama_predict import llama_promt\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "from langdetect import detect\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import nltk\n",
    "\n",
    "from ai.transcriptor.Exctract_keywords import get_keywords, combined_stop_words\n",
    "from ai.transcriptor.STT import speech2text\n",
    "from ai.transcriptor.timecodes_and_full_text import timecodes_and_full_text\n",
    "\n",
    "lang = 'Русский'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nikolaibabuhin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "\n",
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "model_t = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T07:24:19.007816Z",
     "start_time": "2023-11-25T07:24:04.360800Z"
    }
   },
   "id": "a705b097e37becb3"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "d_langs_t = {'Английский': 'eng_Latn',\n",
    "             'Датский': 'dan_Latn',\n",
    "             'Испанский': 'spa_Latn',\n",
    "             'Итальянский': 'ita_Latn',\n",
    "             'Китайский': 'zho_Hans',\n",
    "             'Немецкий': 'deu_Latn',\n",
    "             'Польский': 'pol_Latn',\n",
    "             'Португальский': 'por_Latn',\n",
    "             'Турецкий': 'tur_Latn',\n",
    "             'Французский': 'fra_Latn',\n",
    "             'Чешский': 'ces_Latn',\n",
    "             'Русский': 'rus_Cyrl'\n",
    "             # 'Японский': 'jpn_Jpan',\n",
    "             }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T07:24:19.014585Z",
     "start_time": "2023-11-25T07:24:19.010134Z"
    }
   },
   "id": "dbbaa15c116bc267"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def lemmatize_word(word):\n",
    "    # Определяем язык слова\n",
    "    language = detect(word)\n",
    "\n",
    "    if language == \"en\":\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        return lemmatizer.lemmatize(word)\n",
    "    elif language == \"ru\":\n",
    "        morph = MorphAnalyzer()\n",
    "        return morph.parse(word)[0].normal_form\n",
    "    else:\n",
    "        return word  # Если язык не определен или не поддерживается"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T07:24:19.020467Z",
     "start_time": "2023-11-25T07:24:19.013632Z"
    }
   },
   "id": "9ba07f4fefedab8c"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\n",
    "# Define a function to remove similar words\n",
    "def remove_similar_words(keywords):\n",
    "    import pymorphy2\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    unique_keywords = []\n",
    "    for keyword in keywords:\n",
    "        is_similar = False\n",
    "        for unique_keyword in unique_keywords:\n",
    "            if fuzz.ratio(keyword[0], unique_keyword[0]) > 70:\n",
    "                is_similar = True\n",
    "                break\n",
    "        if not is_similar:\n",
    "            #teg = morph.parse(keyword[0])[0]\n",
    "            unique_keywords.append((lemmatize_word(keyword[0]), keyword[1]))\n",
    "    return unique_keywords"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T07:24:19.026923Z",
     "start_time": "2023-11-25T07:24:19.018928Z"
    }
   },
   "id": "9a14f9e5fb9029cb"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def translator(text, target_language):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = model_t.generate(**inputs, forced_bos_token_id=tokenizer.lang_code_to_id[target_language])\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T07:24:19.031783Z",
     "start_time": "2023-11-25T07:24:19.026250Z"
    }
   },
   "id": "1c95d76092252c03"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def sound_to_text(audio_file):\n",
    "    lang = 'Русский'\n",
    "    conspect = timecodes_and_full_text(audio_file)\n",
    "    \"\"\"\n",
    "    С тайм кодами разбивка + конспект лекции\n",
    "    timecode_with_text': [[x['start'], x['end'], x['text']] for x in data['segments']],\n",
    "        'text': data['text']\n",
    "    \"\"\"\n",
    "    print(conspect)\n",
    "    text = conspect['text']\n",
    "    termins = get_keywords(text, stopwords=combined_stop_words, keyphrase_ngram_range=(1, 1), top_n=200)\n",
    "    # Remove similar words\n",
    "    termins = remove_similar_words(termins)\n",
    "    print(termins)\n",
    "    \"\"\"[('python', 0.4077), ('программирования', 0.31), ('разработчика', 0.2249)]\"\"\"\n",
    "    out = []\n",
    "    \"\"\"for x in termins:\n",
    "        if len(x[0]) < 499:\n",
    "            out.append(translator(llama_promt(x[0]), d_langs_t.get(lang, 'rus_Cyrl')))\n",
    "        else:\n",
    "            print(\"превышен порог в 500 символов\")\"\"\"\n",
    "    #print(out)\n",
    "    return out, conspect, termins"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T07:24:19.032103Z",
     "start_time": "2023-11-25T07:24:19.028662Z"
    }
   },
   "id": "bce9163e0224940a"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Создайте DataFrame с данными для каждой колонки\n",
    "data = {\n",
    "    'File': [],\n",
    "    'Term': []\n",
    "}\n",
    "\n",
    "# Создайте DataFrame из словаря данных\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Запишите DataFrame в CSV файл\n",
    "df.to_csv('new_test.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-25T07:24:19.037176Z",
     "start_time": "2023-11-25T07:24:19.031306Z"
    }
   },
   "id": "cfbf8153432e9688"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./new_test/audiofiles/audio3.mp3\n",
      "Аудиофайл разбит на сегменты\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikolaibabuhin/PycharmProjects/geekbrains-ai-assistant/venv/lib/python3.9/site-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Укажите путь к папке, в которой находятся файлы\n",
    "folder_path = './new_test/audiofiles'\n",
    "\n",
    "# Получите список всех файлов в папке\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "# Переберите каждый файл в списке\n",
    "for file_name in file_list:\n",
    "    # Проверьте, является ли элемент путем к файлу\n",
    "    if os.path.isfile(os.path.join(folder_path, file_name)):\n",
    "        # Выведите ссылку на файл\n",
    "        print(os.path.join(folder_path, file_name))\n",
    "        out, conspect, termins = sound_to_text(os.path.join(folder_path, file_name))\n",
    "        termins\n",
    "        for x in termins:\n",
    "                # Создайте новую строку, которую хотите добавить\n",
    "            new_row = {'File': folder_path,\n",
    "                       'Term': translator(llama_promt(x[0]), d_langs_t.get(lang, 'rus_Cyrl'))}\n",
    "            df = df.append(new_row, ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-25T07:24:19.038620Z"
    }
   },
   "id": "324a6e0831064a6f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
